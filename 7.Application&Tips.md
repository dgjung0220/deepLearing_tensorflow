#### Application& Tips

#### Gradient Descent
기본적으로 0.01 정도로 시작해서 조정한다.

![image](https://user-images.githubusercontent.com/16290330/40432339-6b4dde5e-5ee4-11e8-8f39-361213f3604b.png)

* Large learning rate : Overshooting, 최소값을 뛰어 넘어 오히려 값이 커진다.
* Small learning rate : Takes too long, stops at local minimum, 최소값을 찾는 데 너무 오래 걸리거나, 지역 최소값에 머무른 채 끝날 수 있다. cost 값을 주기적으로 찍어 변화량을 확인해야 한다.

#### Data 정규화
![image](https://user-images.githubusercontent.com/16290330/40432525-e0d40a22-5ee4-11e8-8011-a0ac41737923.png)
* 값 사이가 차이가 많이 날 경우, 등고선이 길게 그려지는 왜곡된 형태로 그려진다. 이를 해결하기 위해 데이터를 normalize 하는 과정이 필요. (data preprocessing)
![image](https://user-images.githubusercontent.com/16290330/40432615-15ff360e-5ee5-11e8-8e22-ed588522fde0.png)
----
![image](https://user-images.githubusercontent.com/16290330/40432684-3968c8da-5ee5-11e8-92e9-b19a8d193811.png)

#### Overfitting
Our model is very good with training data set. Not good at test dataset or in real use.
![image](https://user-images.githubusercontent.com/16290330/40433053-0d2bb7ea-5ee6-11e8-9e47-af05f053ae50.png)
* More training data
* Reduce number of feature
* Regularization (일반화)
* Early stopping
* Dropout

#### Regularization
![image](https://user-images.githubusercontent.com/16290330/40433202-62cf378a-5ee6-11e8-8028-11425a2709c8.png)
