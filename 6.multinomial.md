#### Multinomial 개념

![image](https://user-images.githubusercontent.com/16290330/40430588-6c03934c-5ee0-11e8-83c5-be5aa417c1a5.png)
---
![image](https://user-images.githubusercontent.com/16290330/40430651-91ce1610-5ee0-11e8-8427-1902e508fdb4.png)
---
![image](https://user-images.githubusercontent.com/16290330/40430923-2fff50d8-5ee1-11e8-8f69-3486452f3d97.png)
---
![image](https://user-images.githubusercontent.com/16290330/40430985-5afdcca6-5ee1-11e8-9574-a9299c30ed61.png)
---

#### Softmax
![image](https://user-images.githubusercontent.com/16290330/40431046-852c5024-5ee1-11e8-84ff-6c507aba6bf4.png)
* 0 ~ 1 사이의 값
* 전체의 Sum이 1 (확률로서의 의미를 지닌다.)
```python
hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)
```
#### One-hot Encoding
가장 큰 값만 1.0, 나머지는 0으로 선택 분류. (argmax)
![image](https://user-images.githubusercontent.com/16290330/40431221-f20f1b68-5ee1-11e8-85e8-0f62489420bc.png)
---
![image](https://user-images.githubusercontent.com/16290330/40431328-2b4ab838-5ee2-11e8-8301-5d82a54ec967.png)
---
![image](https://user-images.githubusercontent.com/16290330/40431444-6c4523f0-5ee2-11e8-9c50-9fe1b4919c15.png)
---
#### Logistic cost function vs. Cross Entropy
![image](https://user-images.githubusercontent.com/16290330/40431542-9d19db06-5ee2-11e8-893b-26619419e630.png)
```python
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)
```