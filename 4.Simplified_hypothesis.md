#### Simplified hypothesis

![image](https://user-images.githubusercontent.com/16290330/40426694-b2538118-5ed6-11e8-9fdc-400ffe5e6235.png)

ex)

X | Y
-- | --
1 | 1
2 | 2
3 | 3

W = 1 일 때, cost(W) = 0
W = 0 일 때, cost(W) = 4.67
W = 2 일 때, cost(W) = 4.67

> W = 0, 2 일 때, cost가 같다.

![image](https://user-images.githubusercontent.com/16290330/40427767-95dd47a0-5ed9-11e8-97d4-0899e6dce64b.png)

#### Gradient descent algorithm
* 기울기 하강법, 경사를 따라 내려가는 알고리즘
* Cost function 을 최소화하는 알고리즘
* 값을 최소화하는 많은 문제에 적용됨

#### Formal definition
![image](https://user-images.githubusercontent.com/16290330/40427913-f85ef090-5ed9-11e8-95e7-6d067e5d534f.png)
* 미분할 때, 계산을 쉽게 해주기 위해서, 보통 1/2 을 곱한다. minimize 하는 경우, 1/2 을 곱해도 사실상 값에 변함 없다.
* 현재 W 값에 이전의 W 에서 aplha 값을 곱한 cost의 미분 값을 뺀 후 대입.
* alpha 값은 최적의 기울기를 찾는 데 동작하는 속도를 지정해주는 값. 보통 0.01을 많이 사용한다.

```python
gradient = tf.reduce_mean((W * X - Y)*X)
descent = W - learning_rate * gradient
update = W.assign(descent)
```

#### Multi-variable linear regression
![image](https://user-images.githubusercontent.com/16290330/40429231-283fc71e-5edd-11e8-9fd5-43436921ed25.png)
![image](https://user-images.githubusercontent.com/16290330/40429282-4786127c-5edd-11e8-80a3-ea5a8aaeb28c.png)

ex) 3-variable, H(x1, x2, x3) = W1*x1 + W2*x2 + W3*x3 + b

![image](https://user-images.githubusercontent.com/16290330/40429316-640bc338-5edd-11e8-9a7d-41ef70a1a0a2.png)
![image](https://user-images.githubusercontent.com/16290330/40429390-9371c69a-5edd-11e8-926c-afdd4f543a0f.png)
![image](https://user-images.githubusercontent.com/16290330/40429431-a5ceb294-5edd-11e8-9f91-3ed6c9411b05.png)

